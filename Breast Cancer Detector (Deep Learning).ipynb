{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Dectector using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv')\n",
    "X = dataset.iloc[:, 1:dataset.shape[1] - 1].values\n",
    "y = dataset.iloc[:, dataset.shape[1] - 1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace all missing values of a column (feature) with the most common value of that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "X = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent').fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use One Hot Encoder to encode the dependent variable to make sure that the model will understand that the value 2 and 4 representing 2 classes: 2 for benign, 4 for malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(categories = 'auto')\n",
    "y = onehotencoder.fit_transform(y.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 15% of the dataset for Test set and 85% for Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply feature scaling in order to avoid the domination between large value variables and small value ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and fit the ANN to the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our ANN, we create 2 hidden layers with 5 nodes for each. Each node in hidden layers has Activation function as Rectified Linear Unit (reLU), Initial weights follow Uniform distribution. The output layer has only one node with Softmax as Activation function. We use Cross Entropy as the lost/cost function together with \"Adam\" optimizer as well as \"Accuracy\" metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize the ANN:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding the input layer and the first hidden layer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NHK81HC\\Desktop\\WPy64-3720\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(units = 5, input_dim = 9, activation = 'relu', kernel_initializer = 'uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding the second hidden layer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 5, activation = 'relu', kernel_initializer = 'uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add the output layer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 2, activation = 'softmax', kernel_initializer = 'uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compile the ANN:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Fit the ANN to the Training set (batch_size = 1 and number of epochs = 10):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NHK81HC\\Desktop\\WPy64-3720\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "594/594 [==============================] - 1s 954us/step - loss: 0.3318 - acc: 0.8923\n",
      "Epoch 2/10\n",
      "594/594 [==============================] - 0s 626us/step - loss: 0.0816 - acc: 0.9697\n",
      "Epoch 3/10\n",
      "594/594 [==============================] - 0s 614us/step - loss: 0.0750 - acc: 0.9697\n",
      "Epoch 4/10\n",
      "594/594 [==============================] - 0s 628us/step - loss: 0.0703 - acc: 0.9731\n",
      "Epoch 5/10\n",
      "594/594 [==============================] - 0s 620us/step - loss: 0.0681 - acc: 0.9781\n",
      "Epoch 6/10\n",
      "594/594 [==============================] - 0s 620us/step - loss: 0.0659 - acc: 0.9747\n",
      "Epoch 7/10\n",
      "594/594 [==============================] - 0s 624us/step - loss: 0.0650 - acc: 0.9747\n",
      "Epoch 8/10\n",
      "594/594 [==============================] - 0s 634us/step - loss: 0.0642 - acc: 0.9781\n",
      "Epoch 9/10\n",
      "594/594 [==============================] - 0s 618us/step - loss: 0.0632 - acc: 0.9781\n",
      "Epoch 10/10\n",
      "594/594 [==============================] - 0s 616us/step - loss: 0.0627 - acc: 0.9798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2641f1fe518>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 1, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Test set results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose our set point to be 0.5, which means that if the probability of the cell to be belign equals or greater than 0.5, then it is predicted to be belign. Otherwise, it would be malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (classifier.predict(X_test) >= 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix gives us insight into our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing all possible numbers of epochs, we finally achieves the results table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "| Number of epochs | Mean Accuracy(%) |\n",
    "|:---------------:|:---------------:|\n",
    "|10|97.81|\n",
    "|20|97.98|\n",
    "|50|98.15|\n",
    "|100|98.48|\n",
    "\n",
    "</center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
